{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n입출력 양식을 \\n준수해 주십시오\\n\\n###  INPUT ###\\nimport pandas as pd\\ninput_data = pd.read_csv(\\'2023_smartFarm_AI_hackathon_dataset.csv\\')\\n\\n{\\n    \\n        Write codes...\\n    \\n    Training model name : model\\n    ex) y_pred = model.predict(X_test)\\n    \\n\\n}\\n\\n\\n### OUTPUT ###\\nprint(\"RMSE:\", rmse)\\nprint(\"R2_score:\", r2score)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  -------------------------------------------\n",
    "\"\"\"\n",
    "입출력 양식을 \n",
    "준수해 주십시오\n",
    "\n",
    "###  INPUT ###\n",
    "import pandas as pd\n",
    "input_data = pd.read_csv('2023_smartFarm_AI_hackathon_dataset.csv')\n",
    "\n",
    "{\n",
    "    \n",
    "        Write codes...\n",
    "    \n",
    "    Training model name : model\n",
    "    ex) y_pred = model.predict(X_test)\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "### OUTPUT ###\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2_score:\", r2score)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn import linear_model\n",
    "import sklearn.metrics as sklm\n",
    "\n",
    "import numpy.random as nr\n",
    "import scipy.stats as ss\n",
    "import math\n",
    "\n",
    "import time\n",
    "from keras.callbacks import EarlyStopping  \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn import linear_model\n",
    "\n",
    "# def print_metrics(y_true, y_predicted, n_parameters):\n",
    "#     ## First compute R^2 and the adjusted R^2\n",
    "#     r2 = sklm.r2_score(y_true, y_predicted)\n",
    "#     r2_adj = r2 - (n_parameters - 1)/(y_true.shape[0] - n_parameters) * (1 - r2)\n",
    "\n",
    "\n",
    "#     ## Print the usual metrics and the R^2 values\n",
    "#     # print('Mean Square Error      = ' + str(sklm.mean_squared_error(y_true, y_predicted)))\n",
    "#     print('Root Mean Square Error = ' + str(math.sqrt(sklm.mean_squared_error(y_true, y_predicted))))\n",
    "#     # print('Mean Absolute Error    = ' + str(sklm.mean_absolute_error(y_true, y_predicted)))\n",
    "#     # print('Median Absolute Error  = ' + str(sklm.median_absolute_error(y_true, y_predicted)))\n",
    "#     print('R^2                    = ' + str(r2*100))\n",
    "#     # print('Adjusted R^2           = ' + str(r2_adj))\n",
    "\n",
    "#     return sklm.mean_squared_error(y_true, y_predicted), r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_input2(df):\n",
    "    Features = df\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    n_future = 1  # Number of days we want to predict into the future\n",
    "    n_past = 1  # Number of past days we want to use to predict the future\n",
    "\n",
    "    labels = np.array([row[0] for row in Features])\n",
    "\n",
    "    indx = range(Features.shape[0])\n",
    "    indx = ms.train_test_split(indx, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # Features_train_x = sc.fit_transform(Features[0:len(indx[0]) + 1, 1:16])\n",
    "    # Features_train_y = sc.fit_transform(Features[0:len(indx[0]) + 1, 0:2])\n",
    "\n",
    "    Features_train_x = sc.fit_transform(Features[0:len(indx[0]) + 1, 2:5])\n",
    "    Features_train_y = sc.fit_transform(Features[0:len(indx[0]) + 1, 0:2])  # 2개의 y 변수를 포함하도록 수정\n",
    "\n",
    "    Features_train = np.hstack([Features_train_y, Features_train_x])\n",
    "\n",
    "    Features_test_x = sc.fit_transform(Features[len(indx[0]) - n_past:len(Features), 2:5])\n",
    "    Features_test_y = sc.fit_transform(Features[len(indx[0]) - n_past:len(Features), 0:2])  # 2개의 y 변수를 포함하도록 수정\n",
    "\n",
    "    Features_test = np.hstack([Features_test_y, Features_test_x])\n",
    "\n",
    "    for i in range(n_past, len(indx[0]) - n_future + 1):\n",
    "        x_train.append(Features_train[i - n_past:i, :])  # 모든 feature를 포함하도록 수정\n",
    "        y_train.append(Features_train[i:i + n_future, 0:2])  # 2개의 y 변수를 포함하도록 수정\n",
    "\n",
    "    for i in range(n_past, n_past + len(indx[1])):\n",
    "        x_test.append(Features_test[i - n_past:i, :])  # 모든 feature를 포함하도록 수정\n",
    "        y_test.append(Features_test[i:i + n_future, 0:2])  # 2개의 y 변수를 포함하도록 수정\n",
    "\n",
    "    x_train, y_train, x_test, y_test = np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def LSTM_input(df):\n",
    "    \n",
    "#     Features=df\n",
    "#     x_train = []\n",
    "#     y_train = []\n",
    "#     x_test = []\n",
    "#     y_test = []\n",
    "\n",
    "#     n_future = 1  # Number of days we want top predict into the future\n",
    "#     n_past = 16 # Number of past days we want to use to predict the future\n",
    "\n",
    "#     labels = np.array([row[0] for row in Features])\n",
    "#     # print(labels.shape)\n",
    "    \n",
    "#     indx = range(Features.shape[0])\n",
    "#     indx = ms.train_test_split(indx, test_size = 0.2, shuffle=False)\n",
    "\n",
    "#     #Features_train = sc.fit_transform(Features[0:len(indx[0])+1,:])\n",
    "#     #Features_test = sc.fit_transform(Features[len(indx[0])-n_past:len(Features),:])\n",
    "#     Features_train_x = sc.fit_transform(Features[0:len(indx[0])+1,1:16])\n",
    "#     Features_train_y = sc.fit_transform(Features[0:len(indx[0])+1,0:1])\n",
    "#     print(Features_train_y)\n",
    "#     Features_train = np.hstack([Features_train_y,Features_train_x])\n",
    "\n",
    "#     Features_test_x = sc.fit_transform(Features[len(indx[0])-n_past:len(Features),1:16])\n",
    "#     Features_test_y = sc.fit_transform(Features[len(indx[0])-n_past:len(Features),0:1])\n",
    "\n",
    "#     Features_test = np.hstack([Features_test_y,Features_test_x])\n",
    "\n",
    "#     for i in range(n_past, len(indx[0]) - n_future +1):\n",
    "#         x_train.append(Features_train[i - n_past:i, 0:Features.shape[1]])\n",
    "#         y_train.append(Features_train[i:i + n_future ,0])\n",
    "\n",
    "#     for i in range(n_past, n_past+len(indx[1])):\n",
    "#         x_test.append(Features_test[i - n_past :i, 0:Features.shape[1]])\n",
    "#         y_test.append(Features_test[i:i + n_future,0])\n",
    "\n",
    "#     x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "#     x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "#     # comment out this box and uncomment load_model to load saved model\n",
    "#     ###################################################################################\n",
    "\n",
    "#     return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro2(df):\n",
    "    input_data = df\n",
    "    # input_data=input_data.groupby(\"frmDist\")\n",
    "    # flist = input_data[\"frmDist\"].unique()\n",
    "\n",
    "    # for p in flist[:1]:\n",
    "    #     input_data.get_group(p)\n",
    "    input_data.loc[input_data[\"outtrn_cumsum\"]==0,\"outtrn_cumsum\"]=None\n",
    "    input_data[\"outtrn_cumsum\"]=input_data[\"outtrn_cumsum\"].fillna(method='ffill' or 'pad')\n",
    "    # input_data=input_data.dropna(subset=['outtrn_cumsum'])\n",
    "    input_data.loc[input_data[\"HeatingEnergyUsage_cumsum\"]==0,\"HeatingEnergyUsage_cumsum\"]=None\n",
    "    input_data[\"HeatingEnergyUsage_cumsum\"]=input_data[\"HeatingEnergyUsage_cumsum\"].fillna(method='ffill' or 'pad')\n",
    "    \n",
    "    # 0 값을 NaN으로 바꾸기\n",
    "    input_data = input_data.replace(0, np.nan)\n",
    "    # frtstCo(열매 수) 누적값으로 바꾸기\n",
    "    input_data['frtstCo'] = input_data['frtstCo'].cumsum()\n",
    "    input_data['outTp_minus_inTp'] = input_data['outTp_minus_inTp'].cumsum()\n",
    "    # input_data=input_data.dropna(subset=['HeatingEnergyUsage_cumsum'])\n",
    "    input_data = input_data.interpolate()\n",
    "    input_data = input_data.replace(np.nan,0)\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(df):\n",
    "    input_data = df\n",
    "    # input_data=input_data.groupby(\"frmDist\")\n",
    "    # flist = input_data[\"frmDist\"].unique()\n",
    "\n",
    "    # for p in flist[:1]:\n",
    "    #     input_data.get_group(p)\n",
    "    input_data.loc[input_data[\"outtrn_cumsum\"]==0,\"outtrn_cumsum\"]=None\n",
    "    input_data[\"outtrn_cumsum\"]=input_data[\"outtrn_cumsum\"].fillna(method='ffill' or 'pad')\n",
    "    # input_data=input_data.dropna(subset=['outtrn_cumsum'])\n",
    "    input_data.loc[input_data[\"HeatingEnergyUsage_cumsum\"]==0,\"HeatingEnergyUsage_cumsum\"]=None\n",
    "    input_data[\"HeatingEnergyUsage_cumsum\"]=input_data[\"HeatingEnergyUsage_cumsum\"].fillna(method='ffill' or 'pad')\n",
    "    \n",
    "    # 0 값을 NaN으로 바꾸기\n",
    "    input_data = input_data.replace(0, np.nan)\n",
    "\n",
    "    # input_data=input_data.dropna(subset=['HeatingEnergyUsage_cumsum'])\n",
    "    input_data = input_data.interpolate()\n",
    "    input_data = input_data.replace(np.nan,0)\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4242/4242 [==============================] - 27s 6ms/step - loss: 0.0080 - val_loss: 0.0055\n",
      "Epoch 2/5\n",
      "4242/4242 [==============================] - 26s 6ms/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 3/5\n",
      "4242/4242 [==============================] - 26s 6ms/step - loss: 0.0057 - val_loss: 0.0054\n",
      "Epoch 4/5\n",
      "4242/4242 [==============================] - 26s 6ms/step - loss: 0.0057 - val_loss: 0.0054\n",
      "Epoch 5/5\n",
      "4242/4242 [==============================] - 27s 6ms/step - loss: 0.0056 - val_loss: 0.0054\n",
      "531/531 [==============================] - 2s 3ms/step\n",
      "RMSE: 127581.06712256059\n",
      "R2_score: 0.9268537300556202\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "#      SAMPLE MODEL\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "###  INPUT ###\n",
    "\"\"\"\n",
    "\n",
    "Read CSV files from the given list of file paths and return DataFrames.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "input_data = pd.read_csv(\"2023_smartFarm_AI_hackathon_dataset.csv\")\n",
    "#  -------------------------------------------\n",
    "###    Write codes...  ###\n",
    "#      EXAMPLE CODE      #\n",
    "df_delna = input_data.drop(['daysuplyqy', 'lefstalklt', 'frtstSetCo', 'pllnLt', 'flanJnt', 'hvstJnt', 'flwrCo', 'frtstJnt'], axis=1)\n",
    "df_delna.sort_values(by=[\"frmDist\",\"date\"],ascending=True,inplace=True)\n",
    "jflist=df_delna[\"frmDist\"].unique()\n",
    "fj_env=df_delna.groupby(\"frmDist\")\n",
    "\n",
    "# Read CSV files from the lists of file paths\n",
    "\n",
    "\n",
    "# Now you have lists of DataFrames for each type of data \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "#     X = df[df.drop(columns=['outtrn_cumsum','HeatingEnergyUsage_cumsum']).columns]\n",
    "#     Y = df[['outtrn_cumsum','HeatingEnergyUsage_cumsum']]\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X, Y, test_size=0.2, random_state=42\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "# ... (Data preprocessing code here)\n",
    "\n",
    "flist = input_data[\"frmDist\"].unique()\n",
    "input_data=input_data.drop(['daysuplyqy', 'lefstalklt', 'frtstSetCo', 'pllnLt', 'flanJnt', 'hvstJnt', 'flwrCo', 'frtstJnt'],axis=1)\n",
    "input_data.sort_values(by=[\"frmDist\",\"date\"],ascending=True,inplace=True)\n",
    "\n",
    "input_data = input_data.copy()\n",
    "\n",
    "input_data['inTp'].replace(0, np.nan, inplace=True)\n",
    "input_data['outTp'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "input_data['mmdd'] = input_data['date'].astype(str).str[4:]\n",
    "\n",
    "# 'mmdd'를 기준으로 그룹화하여 변수값의 평균을 계산하고 대체\n",
    "input_data['inTp'].fillna(input_data.groupby('mmdd')['inTp'].transform('mean'), inplace=True)\n",
    "input_data['outTp'].fillna(input_data.groupby('mmdd')['outTp'].transform('mean'), inplace=True)\n",
    "\n",
    "# input_data_r.drop(['mmdd'], axis=1, inplace=True)\n",
    "input_data['inTp'].fillna(0, inplace=True)\n",
    "input_data['outTp'].fillna(0, inplace=True)\n",
    "\n",
    "# 뺀 변수 만들기\n",
    "input_data['outTp_minus_inTp'] = input_data['inTp'] - input_data['outTp']\n",
    "\n",
    "\n",
    "empty_df = pd.DataFrame()\n",
    "for p in flist:\n",
    "    df=fj_env.get_group(p)\n",
    "    # \"inTp\", \"outTp\", \"inHd\",\"inCo2\",\"outWs\",\"acSlrdQy\",\"ec\",\"ph\",\n",
    "    df=df.reindex(columns=[\"date\",\"outtrn_cumsum\",'HeatingEnergyUsage_cumsum','outTp_minus_inTp',\n",
    "                        'frtstCo'\n",
    "                        ])\n",
    "    df=prepro2(df)\n",
    "    # print(df)\n",
    "    df=df.set_index(\"date\")\n",
    "    empty_df=pd.concat([empty_df, df], axis = 0)\n",
    "    # Initialize and train the LinearRegression model\n",
    "empty_df=empty_df.reset_index()\n",
    "dataset_train_actual = empty_df.copy()\n",
    "dataset_train_actual['date']= dataset_train_actual['date'].astype('str')\n",
    "dataset_train_actual['date']=pd.to_datetime(dataset_train_actual['date'])\n",
    "dataset_train_timeindex = dataset_train_actual.set_index('date')\n",
    "\n",
    "# cols = list(dataset_train)[1:24]\n",
    "\n",
    "# datelist_train 그림그리기위한 시간 열\n",
    "# datelist_train = list(dataset_train['date'])\n",
    "# datelist_train = [date for date in datelist_train]\n",
    "\n",
    "# #     print('Training set shape == {}'.format(dataset_train.shape))\n",
    "# #     print('All timestamps == {}'.format(len(datelist_train)))\n",
    "# #     print('Featured selected: {}'.format(cols))\n",
    "# # dataset_train -> copy본\n",
    "#---------------------------------------------------------------------\n",
    "### 변수 나누는 부분\n",
    "dataset_train = dataset_train_actual.copy()\n",
    "\n",
    "# # training and predictions 포함된 열 선택\n",
    "cols = list(dataset_train)[1:5]\n",
    "# print(cols)\n",
    "# # # datelist_train 그림그리기위한 시간 열\n",
    "datelist_train = list(dataset_train['date'])\n",
    "datelist_train = [date for date in datelist_train]\n",
    "dataset_train = dataset_train[cols].astype(str)\n",
    "# # # for i in cols:\n",
    "# #     for j in range(0, len(dataset_train)):\n",
    "# #         dataset_train[i][j] = dataset_train[i][j].replace(',', '')\n",
    "\n",
    "# # datelist_train float 타입으로 바꿔주기\n",
    "dataset_train = dataset_train.astype(float)\n",
    "\n",
    "# # 여러 feature값 (predictors)\n",
    "training_set = dataset_train.values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "# train_set_scale = sc.fit_transform(training_set[:,1:])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# train_set_scale=np.append(training_set[:, 0:1],train_set_scale,axis=1)\n",
    "# train_set_scale\n",
    "\n",
    "# x_train, y_train, x_test, y_test = LSTM_input2(train_set_scale)\n",
    "x_train, y_train, x_test, y_test = LSTM_input2(training_set)\n",
    "    \n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(int(32), input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(2,activation='relu'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x_train,y_train,\n",
    "          epochs=5,\n",
    "          batch_size=16,\n",
    "          validation_data=(x_test, y_test),\n",
    "          verbose=1,\n",
    "          shuffle=False,\n",
    "          # callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)]\n",
    "         )\n",
    "y_pred = sc.inverse_transform(model.predict(x_test))\n",
    "# print(y_pred.shape)\n",
    "# print(y_test.shape)\n",
    "# Calculate RMSE between the predictions and actual 'y' values\n",
    "\n",
    "def calculate_rmse(targets, predictions):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Squared Error (RMSE) between predicted and target values.\n",
    "\n",
    "    :param predictions: Predicted values.\n",
    "    :type predictions: array-like\n",
    "    :param targets: Target values.\n",
    "    :type targets: array-like\n",
    "    :return: RMSE value.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return np.sqrt(mean_squared_error(targets, predictions))\n",
    "\n",
    "\n",
    "# Calculate r2_score between the predictions and actual 'y' values\n",
    "def calculate_R2_score(y_test,y_score):\n",
    "    # n_parameters=x_train.shape[1]\n",
    "    # y_score = sc.inverse_transform(model.predict(x_test))\n",
    "    # print(y_score.shape)\n",
    "    # rmse, r2score = print_metrics(sc.inverse_transform(y_test), y_score, n_parameters)\n",
    "    # print(\"RMSE:\", rmse)\n",
    "    # print(\"R2_score:\", r2score)\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2 = sklm.r2_score(y_test, y_score)\n",
    "    return r2\n",
    "\n",
    "y_train=y_train.reshape(-1,2)\n",
    "y_test=y_test.reshape(-1,2)\n",
    "y_test=pd.DataFrame(sc.inverse_transform(y_test),columns = ['outtrn_cumsum','HeatingEnergyUsage_cumsum'])\n",
    "rmse = calculate_rmse(y_test, y_pred)\n",
    "r2score = calculate_R2_score(y_test, y_pred)\n",
    "# Predict 'y' values using the trained model\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# ------------------------------------------------\n",
    "### OUTPUT ###\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2_score:\", r2score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       outtrn_cumsum  HeatingEnergyUsage_cumsum\n",
       " 0      175999.318985                        0.0\n",
       " 1      172914.415459                        0.0\n",
       " 2      178163.786685                        0.0\n",
       " 3      179110.984486                        0.0\n",
       " 4      179110.984486                        0.0\n",
       " ...              ...                        ...\n",
       " 16963  235272.042536                        0.0\n",
       " 16964  227947.124946                        0.0\n",
       " 16965  231527.426327                        0.0\n",
       " 16966  251254.321635                        0.0\n",
       " 16967  251254.321635                        0.0\n",
       " \n",
       " [16968 rows x 2 columns],\n",
       " array([[163181.98,      0.  ],\n",
       "        [165240.38,      0.  ],\n",
       "        [162822.11,      0.  ],\n",
       "        ...,\n",
       "        [202860.25,      0.  ],\n",
       "        [205236.12,      0.  ],\n",
       "        [217825.81,      0.  ]], dtype=float32))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kty",
   "language": "python",
   "name": "kty"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
