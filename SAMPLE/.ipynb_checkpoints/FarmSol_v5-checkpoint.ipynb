{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023 스마트농업 AI 경진대회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-17 07:41:17.916801: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-17 07:41:17.919019: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-17 07:41:17.959758: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-17 07:41:17.961027: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-17 07:41:18.631362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, ThresholdedReLU, ReLU, Activation\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn import linear_model\n",
    "import sklearn.metrics as sklm\n",
    "\n",
    "import numpy.random as nr\n",
    "import scipy.stats as ss\n",
    "import math\n",
    "\n",
    "import time\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn import linear_model\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_input2(df):\n",
    "    Features = df\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    n_future = 1  # Number of days we want to predict into the future\n",
    "    n_past = 1  # Number of past days we want to use to predict the future\n",
    "\n",
    "    labels = np.array([row[0] for row in Features])\n",
    "\n",
    "    indx = range(Features.shape[0])\n",
    "    indx = ms.train_test_split(indx, test_size=0.2, shuffle=False)\n",
    "\n",
    "    Features_train_x = sc.fit_transform(Features[0:len(indx[0]) + 1, 2:7])\n",
    "    Features_train_y = sc.fit_transform(Features[0:len(indx[0]) + 1, 0:2])  # 2개의 y 변수를 포함하도록 수정\n",
    "\n",
    "    Features_train = np.hstack([Features_train_y, Features_train_x])\n",
    "\n",
    "    Features_test_x = sc.fit_transform(Features[len(indx[0]) - n_past:len(Features), 2:7])\n",
    "    Features_test_y = sc.fit_transform(Features[len(indx[0]) - n_past:len(Features), 0:2])  # 2개의 y 변수를 포함하도록 수정\n",
    "\n",
    "    Features_test = np.hstack([Features_test_y, Features_test_x])\n",
    "\n",
    "    for i in range(n_past, len(indx[0]) - n_future + 1):\n",
    "        x_train.append(Features_train[i - n_past:i, :])  # 모든 feature를 포함하도록 수정\n",
    "        y_train.append(Features_train[i:i + n_future, 0:2])  # 2개의 y 변수를 포함하도록 수정\n",
    "\n",
    "    for i in range(n_past, n_past + len(indx[1])):\n",
    "        x_test.append(Features_test[i - n_past:i, :])  # 모든 feature를 포함하도록 수정\n",
    "        y_test.append(Features_test[i:i + n_future, 0:2])  # 2개의 y 변수를 포함하도록 수정\n",
    "\n",
    "    x_train, y_train, x_test, y_test = np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro2(df):\n",
    "    input_data = df\n",
    "    \n",
    "    input_data.loc[input_data[\"outtrn_cumsum\"]==0,\"outtrn_cumsum\"]=None\n",
    "    input_data[\"outtrn_cumsum\"]=input_data[\"outtrn_cumsum\"].fillna(method='ffill' or 'pad')\n",
    "    \n",
    "    input_data.loc[input_data[\"HeatingEnergyUsage_cumsum\"]==0,\"HeatingEnergyUsage_cumsum\"]=None\n",
    "    input_data[\"HeatingEnergyUsage_cumsum\"]=input_data[\"HeatingEnergyUsage_cumsum\"].fillna(method='ffill' or 'pad')\n",
    "    \n",
    "    input_data.loc[input_data[\"acslrdqy\"]==0,\"acslrdqy\"]=None\n",
    "    input_data[\"acslrdqy\"]=input_data[\"acslrdqy\"].fillna(method='ffill' or 'pad')\n",
    "    \n",
    "    input_data.loc[input_data[\"FertilizerUsage\"]==0,\"FertilizerUsage\"]=None\n",
    "    input_data[\"FertilizerUsage\"]=input_data[\"FertilizerUsage\"].fillna(method='ffill' or 'pad')\n",
    "    \n",
    "    # 0 값을 NaN으로 바꾸기\n",
    "    input_data = input_data.replace(0, np.nan)\n",
    "    \n",
    "    # frtstCo(열매 수) 누적값으로 바꾸기\n",
    "    input_data['frtstCo'] = input_data['frtstCo'].cumsum()\n",
    "    # outTp_minus_inTp 변수 생성\n",
    "    input_data['outTp_minus_inTp'] = input_data['outTp_minus_inTp'].cumsum()\n",
    "    input_data = input_data.interpolate()\n",
    "    input_data = input_data.replace(np.nan,0)\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-17 07:41:29.299056: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "8484/8484 [==============================] - 49s 6ms/step - loss: 0.0048 - val_loss: 0.0041\n",
      "Epoch 2/300\n",
      "8484/8484 [==============================] - 44s 5ms/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 3/300\n",
      "8484/8484 [==============================] - 47s 6ms/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 4/300\n",
      "8484/8484 [==============================] - 47s 5ms/step - loss: 0.0034 - val_loss: 0.0029\n",
      "Epoch 5/300\n",
      "7208/8484 [========================>.....] - ETA: 6s - loss: 0.0034"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "###  INPUT ###\n",
    "\"\"\"\n",
    "\n",
    "Read CSV files from the given list of file paths and return DataFrames.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "input_data = pd.read_csv(\"2023_smartFarm_AI_hackathon_dataset.csv\")\n",
    "\n",
    "#  -------------------------------------------\n",
    "###    Write codes...  ###\n",
    "#      EXAMPLE CODE      #\n",
    "df_delna = input_data.drop(['daysuplyqy', 'lefstalklt', 'frtstSetCo', 'pllnLt', 'flanJnt', 'hvstJnt', 'flwrCo', 'frtstJnt'], axis=1)\n",
    "df_delna.sort_values(by=[\"frmDist\",\"date\"],ascending=True,inplace=True)\n",
    "jflist=df_delna[\"frmDist\"].unique()\n",
    "fj_env=df_delna.groupby(\"frmDist\")\n",
    "\n",
    "# Read CSV files from the lists of file paths\n",
    "\n",
    "# Now you have lists of DataFrames for each type of data \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ... (Data preprocessing code here)\n",
    "\n",
    "flist = input_data[\"frmDist\"].unique()\n",
    "input_data=input_data.drop(['daysuplyqy', 'lefstalklt', 'frtstSetCo', 'pllnLt', 'flanJnt', 'hvstJnt', 'flwrCo', 'frtstJnt'],axis=1)\n",
    "input_data.sort_values(by=[\"frmDist\",\"date\"],ascending=True,inplace=True)\n",
    "\n",
    "input_data = input_data.copy()\n",
    "\n",
    "input_data['inTp'].replace(0, np.nan, inplace=True)\n",
    "input_data['outTp'].replace(0, np.nan, inplace=True)\n",
    "input_data.loc[input_data['date'] >= 20221102, 'inTp'] = None\n",
    "input_data.loc[input_data['date'] >= 20221102, 'outTp'] = None\n",
    "\n",
    "input_data['mmdd'] = input_data['date'].astype(str).str[4:]\n",
    "\n",
    "# 'mmdd'를 기준으로 그룹화하여 변수값의 평균을 계산하고 대체\n",
    "input_data['inTp'].fillna(input_data.groupby('mmdd')['inTp'].transform('mean'), inplace=True)\n",
    "input_data['outTp'].fillna(input_data.groupby('mmdd')['outTp'].transform('mean'), inplace=True)\n",
    "\n",
    "# input_data_r.drop(['mmdd'], axis=1, inplace=True)\n",
    "input_data['inTp'].fillna(0, inplace=True)\n",
    "input_data['outTp'].fillna(0, inplace=True)\n",
    "\n",
    "# 뺀 변수 만들기\n",
    "input_data['outTp_minus_inTp'] = input_data['inTp'] - input_data['outTp']\n",
    "\n",
    "empty_df = pd.DataFrame()\n",
    "for p in flist:\n",
    "    df=fj_env.get_group(p)\n",
    "    # \"inTp\", \"outTp\", \"inHd\",\"inCo2\",\"outWs\",\"acSlrdQy\",\"ec\",\"ph\",\n",
    "    df=df.reindex(columns=[\"date\",\"outtrn_cumsum\",'HeatingEnergyUsage_cumsum','outTp_minus_inTp',\n",
    "                        'frtstCo','acslrdqy','FertilizerUsage'\n",
    "                        ])\n",
    "    df=prepro2(df)\n",
    "    # print(df)\n",
    "    df=df.set_index(\"date\")\n",
    "    empty_df=pd.concat([empty_df, df], axis = 0)\n",
    "    # Initialize and train the LinearRegression model\n",
    "empty_df=empty_df.reset_index()\n",
    "dataset_train_actual = empty_df.copy()\n",
    "dataset_train_actual['date']= dataset_train_actual['date'].astype('str')\n",
    "dataset_train_actual['date']=pd.to_datetime(dataset_train_actual['date'])\n",
    "dataset_train_timeindex = dataset_train_actual.set_index('date')\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "### 변수 나누는 부분\n",
    "dataset_train = dataset_train_actual.copy()\n",
    "\n",
    "# # training and predictions 포함된 열 선택\n",
    "cols = list(dataset_train)[1:7]\n",
    "\n",
    "datelist_train = list(dataset_train['date'])\n",
    "datelist_train = [date for date in datelist_train]\n",
    "dataset_train = dataset_train[cols].astype(str)\n",
    "\n",
    "# # datelist_train float 타입으로 바꿔주기\n",
    "dataset_train = dataset_train.astype(float)\n",
    "\n",
    "# # 여러 feature값 (predictors)\n",
    "training_set = dataset_train.values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "\n",
    "x_train, y_train, x_test, y_test = LSTM_input2(training_set)\n",
    "    \n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(int(32), input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "model.add(Dropout(.3))\n",
    "# model.add(ThresholdedReLU(theta=100.0))\n",
    "# model.add(Dense(2,ThresholdedReLU(theta=100)))\n",
    "# ReLU(threshold=100)\n",
    "model.add(Dense(2,activation='relu'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x_train,y_train,\n",
    "          epochs=300,\n",
    "          batch_size=8,\n",
    "          validation_data=(x_test, y_test),\n",
    "          verbose=1,\n",
    "          shuffle=False,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10)]\n",
    "          # callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)]\n",
    "         )\n",
    "y_pred = sc.inverse_transform(model.predict(x_test))\n",
    "\n",
    "# Calculate RMSE between the predictions and actual 'y' values\n",
    "\n",
    "def calculate_rmse(targets, predictions):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Squared Error (RMSE) between predicted and target values.\n",
    "\n",
    "    :param predictions: Predicted values.\n",
    "    :type predictions: array-like\n",
    "    :param targets: Target values.\n",
    "    :type targets: array-like\n",
    "    :return: RMSE value.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return np.sqrt(mean_squared_error(targets, predictions))\n",
    "\n",
    "\n",
    "# Calculate r2_score between the predictions and actual 'y' values\n",
    "def calculate_R2_score(y_test,y_score):\n",
    "    # n_parameters=x_train.shape[1]\n",
    "    # y_score = sc.inverse_transform(model.predict(x_test))\n",
    "    # print(y_score.shape)\n",
    "    # rmse, r2score = print_metrics(sc.inverse_transform(y_test), y_score, n_parameters)\n",
    "    # print(\"RMSE:\", rmse)\n",
    "    # print(\"R2_score:\", r2score)\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2 = sklm.r2_score(y_test, y_score)\n",
    "    return r2\n",
    "\n",
    "y_train=y_train.reshape(-1,2)\n",
    "y_test=y_test.reshape(-1,2)\n",
    "y_test=pd.DataFrame(sc.inverse_transform(y_test),columns = ['outtrn_cumsum','HeatingEnergyUsage_cumsum'])\n",
    "rmse = calculate_rmse(y_test, y_pred)\n",
    "r2score = calculate_R2_score(y_test, y_pred)\n",
    "# Predict 'y' values using the trained model\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# ------------------------------------------------\n",
    "### OUTPUT ###\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2_score:\", r2score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kty",
   "language": "python",
   "name": "kty"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
