{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n입출력 양식을 \\n준수해 주십시오\\n\\n###  INPUT ###\\nimport pandas as pd\\ninput_data = pd.read_csv(\\'2023_smartFarm_AI_hackathon_dataset.csv\\')\\n\\n{\\n    \\n        Write codes...\\n    \\n    Training model name : model\\n    ex) y_pred = model.predict(X_test)\\n    \\n\\n}\\n\\n\\n### OUTPUT ###\\nprint(\"RMSE:\", rmse)\\nprint(\"R2_score:\", r2score)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  -------------------------------------------\n",
    "\"\"\"\n",
    "입출력 양식을 \n",
    "준수해 주십시오\n",
    "\n",
    "###  INPUT ###\n",
    "import pandas as pd\n",
    "input_data = pd.read_csv('2023_smartFarm_AI_hackathon_dataset.csv')\n",
    "\n",
    "{\n",
    "    \n",
    "        Write codes...\n",
    "    \n",
    "    Training model name : model\n",
    "    ex) y_pred = model.predict(X_test)\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "### OUTPUT ###\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2_score:\", r2score)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn import linear_model\n",
    "import sklearn.metrics as sklm\n",
    "\n",
    "import numpy.random as nr\n",
    "import scipy.stats as ss\n",
    "import math\n",
    "\n",
    "import time\n",
    "from keras.callbacks import EarlyStopping  \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn import linear_model\n",
    "\n",
    "# def print_metrics(y_true, y_predicted, n_parameters):\n",
    "#     ## First compute R^2 and the adjusted R^2\n",
    "#     r2 = sklm.r2_score(y_true, y_predicted)\n",
    "#     r2_adj = r2 - (n_parameters - 1)/(y_true.shape[0] - n_parameters) * (1 - r2)\n",
    "\n",
    "\n",
    "#     ## Print the usual metrics and the R^2 values\n",
    "#     # print('Mean Square Error      = ' + str(sklm.mean_squared_error(y_true, y_predicted)))\n",
    "#     print('Root Mean Square Error = ' + str(math.sqrt(sklm.mean_squared_error(y_true, y_predicted))))\n",
    "#     # print('Mean Absolute Error    = ' + str(sklm.mean_absolute_error(y_true, y_predicted)))\n",
    "#     # print('Median Absolute Error  = ' + str(sklm.median_absolute_error(y_true, y_predicted)))\n",
    "#     print('R^2                    = ' + str(r2*100))\n",
    "#     # print('Adjusted R^2           = ' + str(r2_adj))\n",
    "\n",
    "#     return sklm.mean_squared_error(y_true, y_predicted), r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_input2(df):\n",
    "    Features = df\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    n_future = 1  # Number of days we want to predict into the future\n",
    "    n_past = 16  # Number of past days we want to use to predict the future\n",
    "\n",
    "    labels = np.array([row[0] for row in Features])\n",
    "\n",
    "    print(Features.shape)\n",
    "    indx = range(Features.shape[0])\n",
    "    indx = ms.train_test_split(indx, test_size=0.2, shuffle=False)\n",
    "    # print(indx)\n",
    "    # Features_train_x = sc.fit_transform(Features[0:len(indx[0]) + 1, 1:16])\n",
    "    # Features_train_y = sc.fit_transform(Features[0:len(indx[0]) + 1, 0:2])\n",
    "\n",
    "    # print(Features[0:len(indx[0]) + 1, 1:16])\n",
    "    Features_train_x = sc.fit_transform(Features[0:len(indx[0]) + 1, 1:16])\n",
    "    Features_train_y = sc.fit_transform(Features[0:len(indx[0]) + 1, 0:1])  # 2개의 y 변수를 포함하도록 수정\\\n",
    "    # print(Features_train_x.shape)\n",
    "    # print(Features_train_y.shape)\n",
    "    Features_train = np.hstack([Features_train_y, Features_train_x])\n",
    "\n",
    "    Features_test_x = sc.fit_transform(Features[len(indx[0]) - n_past:len(Features), 1:16])\n",
    "    Features_test_y = sc.fit_transform(Features[len(indx[0]) - n_past:len(Features), 0:1])  # 2개의 y 변수를 포함하도록 수정\n",
    "\n",
    "    Features_test = np.hstack([Features_test_y, Features_test_x])\n",
    "\n",
    "    for i in range(n_past, len(indx[0]) - n_future + 1):\n",
    "        x_train.append(Features_train[i - n_past:i, 0:Features.shape[1]])  # 모든 feature를 포함하도록 수정\n",
    "        y_train.append(Features_train[i:i + n_future, 0])  # 2개의 y 변수를 포함하도록 수정\n",
    "\n",
    "    for i in range(n_past, n_past + len(indx[1])):\n",
    "        x_test.append(Features_test[i - n_past:i, 0:Features.shape[1]])  # 모든 feature를 포함하도록 수정\n",
    "        y_test.append(Features_test[i:i + n_future, 0:1])  # 2개의 y 변수를 포함하도록 수정\n",
    "\n",
    "    x_train, y_train, x_test, y_test = np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def LSTM_input(df):\n",
    "    \n",
    "#     Features=df\n",
    "#     x_train = []\n",
    "#     y_train = []\n",
    "#     x_test = []\n",
    "#     y_test = []\n",
    "\n",
    "#     n_future = 1  # Number of days we want top predict into the future\n",
    "#     n_past = 16 # Number of past days we want to use to predict the future\n",
    "\n",
    "#     labels = np.array([row[0] for row in Features])\n",
    "#     # print(labels.shape)\n",
    "    \n",
    "#     indx = range(Features.shape[0])\n",
    "#     indx = ms.train_test_split(indx, test_size = 0.2, shuffle=False)\n",
    "\n",
    "#     #Features_train = sc.fit_transform(Features[0:len(indx[0])+1,:])\n",
    "#     #Features_test = sc.fit_transform(Features[len(indx[0])-n_past:len(Features),:])\n",
    "#     Features_train_x = sc.fit_transform(Features[0:len(indx[0])+1,1:16])\n",
    "#     Features_train_y = sc.fit_transform(Features[0:len(indx[0])+1,0:1])\n",
    "#     print(Features_train_y)\n",
    "#     Features_train = np.hstack([Features_train_y,Features_train_x])\n",
    "\n",
    "#     Features_test_x = sc.fit_transform(Features[len(indx[0])-n_past:len(Features),1:16])\n",
    "#     Features_test_y = sc.fit_transform(Features[len(indx[0])-n_past:len(Features),0:1])\n",
    "\n",
    "#     Features_test = np.hstack([Features_test_y,Features_test_x])\n",
    "\n",
    "#     for i in range(n_past, len(indx[0]) - n_future +1):\n",
    "#         x_train.append(Features_train[i - n_past:i, 0:Features.shape[1]])\n",
    "#         y_train.append(Features_train[i:i + n_future ,0])\n",
    "\n",
    "#     for i in range(n_past, n_past+len(indx[1])):\n",
    "#         x_test.append(Features_test[i - n_past :i, 0:Features.shape[1]])\n",
    "#         y_test.append(Features_test[i:i + n_future,0])\n",
    "\n",
    "#     x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "#     x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "#     # comment out this box and uncomment load_model to load saved model\n",
    "#     ###################################################################################\n",
    "\n",
    "#     return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(df):\n",
    "    input_data = df\n",
    "    # input_data=input_data.groupby(\"frmDist\")\n",
    "    # flist = input_data[\"frmDist\"].unique()\n",
    "\n",
    "    # for p in flist[:1]:\n",
    "    #     input_data.get_group(p)\n",
    "    input_data.loc[input_data[\"outtrn_cumsum\"]==0,\"outtrn_cumsum\"]=None\n",
    "    input_data[\"outtrn_cumsum\"]=input_data[\"outtrn_cumsum\"].fillna(method='ffill' or 'pad')\n",
    "    # input_data=input_data.dropna(subset=['outtrn_cumsum'])\n",
    "    input_data.loc[input_data[\"HeatingEnergyUsage_cumsum\"]==0,\"HeatingEnergyUsage_cumsum\"]=None\n",
    "    input_data[\"HeatingEnergyUsage_cumsum\"]=input_data[\"HeatingEnergyUsage_cumsum\"].fillna(method='ffill' or 'pad')\n",
    "    \n",
    "    # 0 값을 NaN으로 바꾸기\n",
    "    input_data = input_data.replace(0, np.nan)\n",
    "    # frtstCo(열매 수) 누적값으로 바꾸기\n",
    "    input_data['frtstCo'] = input_data['frtstCo'].cumsum()\n",
    "    input_data['outTp_minus_inTp'] = input_data['outTp_minus_inTp'].cumsum()\n",
    "    # input_data=input_data.dropna(subset=['HeatingEnergyUsage_cumsum'])\n",
    "    input_data = input_data.interpolate()\n",
    "    input_data = input_data.replace(np.nan,0)\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [2.31527426e+05, 0.00000000e+00, 2.30969907e+01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.51254322e+05, 0.00000000e+00, 2.30969907e+01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.51254322e+05, 0.00000000e+00, 2.30969907e+01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84840, 27)\n",
      "(84840, 24)\n",
      "2121/2121 [==============================] - 35s 16ms/step - loss: 0.0118 - val_loss: 0.0073\n",
      "531/531 [==============================] - 4s 6ms/step\n",
      "RMSE: 3.1789816289174673\n",
      "R2_score: 0.7832291970717024\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "#      SAMPLE MODEL\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "###  INPUT ###\n",
    "\"\"\"\n",
    "\n",
    "Read CSV files from the given list of file paths and return DataFrames.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "input_data = pd.read_csv(\"2023_smartFarm_AI_hackathon_dataset.csv\")\n",
    "#  -------------------------------------------\n",
    "###    Write codes...  ###\n",
    "#      EXAMPLE CODE      #\n",
    "df_delna = input_data.drop(['daysuplyqy', 'lefstalklt', 'frtstSetCo', 'pllnLt', 'flanJnt', 'hvstJnt', 'flwrCo', 'frtstJnt'], axis=1)\n",
    "df_delna.sort_values(by=[\"frmDist\",\"date\"],ascending=True,inplace=True)\n",
    "jflist=df_delna[\"frmDist\"].unique()\n",
    "fj_env=df_delna.groupby(\"frmDist\")\n",
    "\n",
    "# Read CSV files from the lists of file paths\n",
    "\n",
    "\n",
    "# Now you have lists of DataFrames for each type of data \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ... (Data preprocessing code here)\n",
    "\n",
    "flist = input_data[\"frmDist\"].unique()\n",
    "input_data=input_data.drop(['daysuplyqy', 'lefstalklt', 'frtstSetCo', 'pllnLt', 'flanJnt', 'hvstJnt', 'flwrCo', 'frtstJnt'],axis=1)\n",
    "input_data.sort_values(by=[\"frmDist\",\"date\"],ascending=True,inplace=True)\n",
    "\n",
    "input_data = input_data.copy()\n",
    "\n",
    "input_data['inTp'].replace(0, np.nan, inplace=True)\n",
    "input_data['outTp'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "input_data['mmdd'] = input_data['date'].astype(str).str[4:]\n",
    "\n",
    "# 'mmdd'를 기준으로 그룹화하여 변수값의 평균을 계산하고 대체\n",
    "input_data['inTp'].fillna(input_data.groupby('mmdd')['inTp'].transform('mean'), inplace=True)\n",
    "input_data['outTp'].fillna(input_data.groupby('mmdd')['outTp'].transform('mean'), inplace=True)\n",
    "\n",
    "# input_data_r.drop(['mmdd'], axis=1, inplace=True)\n",
    "input_data['inTp'].fillna(0, inplace=True)\n",
    "input_data['outTp'].fillna(0, inplace=True)\n",
    "\n",
    "# 뺀 변수 만들기\n",
    "input_data['outTp_minus_inTp'] = input_data['inTp'] - input_data['outTp']\n",
    "\n",
    "empty_df = pd.DataFrame()\n",
    "for p in flist:\n",
    "    df=fj_env.get_group(p)\n",
    "    df=df.reindex(columns=[\"date\",\"outtrn_cumsum\",'HeatingEnergyUsage_cumsum',\"inTp\",\"outTp\",\"inHd\",\"inCo2\",\"outWs\",\"acSlrdQy\",\"ec\",\"ph\",\n",
    "                        \"frtstGrupp\",\"flanGrupp\",\"frtstCo\", 'tcdmt', 'frmhsFclu', 'hvstGrupp',\n",
    "                        'grwtLt', 'fcluHg', 'lefLt', 'hvstCo', 'lefCunt',\n",
    "                        'lefBt', 'stemThck', 'frmAr', 'frmDov','outTp_minus_inTp'\n",
    "                        ])\n",
    "    df=prepro(df)\n",
    "    df=df.set_index(\"date\")\n",
    "    empty_df=pd.concat([empty_df, df], axis = 0)\n",
    "    \n",
    "# Initialize and train the LinearRegression model\n",
    "empty_df=empty_df.reset_index()\n",
    "dataset_train_actual = empty_df.copy()\n",
    "dataset_train_actual['date']= dataset_train_actual['date'].astype('str')\n",
    "dataset_train_actual['date']=pd.to_datetime(dataset_train_actual['date'])\n",
    "dataset_train_timeindex = dataset_train_actual.set_index('date')\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "### 변수 나누는 부분\n",
    "dataset_train = dataset_train_actual.copy()\n",
    "\n",
    "# # training and predictions 포함된 열 선택\n",
    "cols = list(dataset_train)[1:25]\n",
    "# print(cols)\n",
    "# # # datelist_train 그림그리기위한 시간 열\n",
    "datelist_train = list(dataset_train['date'])\n",
    "datelist_train = [date for date in datelist_train]\n",
    "dataset_train = dataset_train[cols].astype(str)\n",
    "\n",
    "# # datelist_train float 타입으로 바꿔주기\n",
    "dataset_train = dataset_train.astype(float)\n",
    "\n",
    "# # 여러 feature값 (predictors)\n",
    "training_set = dataset_train.values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# y 종속변수 2개 지정 \n",
    "print(dataset_train_actual.shape)\n",
    "train_set_scale = training_set[:,2:]\n",
    "\n",
    "train_set_scale=np.append(train_set_scale[:, 0:2],train_set_scale,axis=1)\n",
    "train_set_scale\n",
    "\n",
    "# x_train, y_train, x_test, y_test = LSTM_input(train_set_scale)\n",
    "x_train, y_train, x_test, y_test = LSTM_input2(train_set_scale)\n",
    "    \n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(int(32), input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x_train,y_train,\n",
    "          epochs=1,\n",
    "          batch_size=32,\n",
    "          validation_data=(x_test, y_test),\n",
    "          verbose=1,\n",
    "          shuffle=False,\n",
    "          # callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)]\n",
    "         )\n",
    "y_pred = sc.inverse_transform(model.predict(x_test))\n",
    "\n",
    "# Calculate RMSE between the predictions and actual 'y' values\n",
    "\n",
    "def calculate_rmse(targets, predictions):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Squared Error (RMSE) between predicted and target values.\n",
    "\n",
    "    :param predictions: Predicted values.\n",
    "    :type predictions: array-like\n",
    "    :param targets: Target values.\n",
    "    :type targets: array-like\n",
    "    :return: RMSE value.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return np.sqrt(mean_squared_error(targets, predictions))\n",
    "\n",
    "\n",
    "# Calculate r2_score between the predictions and actual 'y' values\n",
    "\n",
    "def calculate_R2_score(y_test,y_score):\n",
    "    # n_parameters=x_train.shape[1]\n",
    "    # y_score = sc.inverse_transform(model.predict(x_test))\n",
    "    # print(y_score.shape)\n",
    "    # rmse, r2score = print_metrics(sc.inverse_transform(y_test), y_score, n_parameters)\n",
    "    # print(\"RMSE:\", rmse)\n",
    "    # print(\"R2_score:\", r2score)\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2 = sklm.r2_score(y_test, y_score)\n",
    "    return r2\n",
    "\n",
    "# 평가지표 계산을 위한 shape 변형\n",
    "y_train=y_train.reshape(-1,1)\n",
    "y_test=y_test.reshape(-1,1)\n",
    "\n",
    "y_test=pd.DataFrame(sc.inverse_transform(y_test))\n",
    "rmse = calculate_rmse(y_test, y_pred)\n",
    "r2score = calculate_R2_score(y_test, y_pred)\n",
    "\n",
    "# ------------------------------------------------\n",
    "### OUTPUT ###\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2_score:\", r2score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.492646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.323035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.576610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.124594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16963</th>\n",
       "      <td>23.096991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16964</th>\n",
       "      <td>23.096991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16965</th>\n",
       "      <td>23.096991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16966</th>\n",
       "      <td>23.096991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16967</th>\n",
       "      <td>23.096991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16968 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0      22.492646\n",
       "1      25.323035\n",
       "2      21.576610\n",
       "3      21.266500\n",
       "4      20.124594\n",
       "...          ...\n",
       "16963  23.096991\n",
       "16964  23.096991\n",
       "16965  23.096991\n",
       "16966  23.096991\n",
       "16967  23.096991\n",
       "\n",
       "[16968 rows x 1 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.901802],\n",
       "       [21.079952],\n",
       "       [21.90747 ],\n",
       "       ...,\n",
       "       [22.041739],\n",
       "       [21.99608 ],\n",
       "       [21.97025 ]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ZipFile.__del__ at 0x7fec07e0f1f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-kty7613/.conda/envs/kty/lib/python3.9/zipfile.py\", line 1816, in __del__\n",
      "    self.close()\n",
      "  File \"/home/jupyter-kty7613/.conda/envs/kty/lib/python3.9/zipfile.py\", line 1833, in close\n",
      "    self.fp.seek(self.start_dir)\n",
      "ValueError: seek of closed file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with pd.ExcelWriter('y.xlsx') as writer:\n",
    "    pd.DataFrame(sc.inverse_transform(y_pred)).to_excel(writer, sheet_name='y_pred')\n",
    "    pd.DataFrame(sc.inverse_transform(y_test)).to_excel(writer, sheet_name='y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kty",
   "language": "python",
   "name": "kty"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
